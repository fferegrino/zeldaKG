{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infobox extraction from wikia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from slugify import slugify\n",
    "from glob import glob\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import unquote\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Where are all those htmls?\n",
    "html_route = r\"C:\\Corpora\\zelda-wikia2-clean\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(url):\n",
    "    \"\"\"\n",
    "    Clean the url to met the structure adopted for the dataset\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    path = unquote(parsed.path)\n",
    "    if path.startswith(\"../\"):\n",
    "        path = path[3:]\n",
    "    path = path.replace(\"/\", \"%2F\")\n",
    "    query = None if parsed.query == '' else parsed.query\n",
    "    fragment = None if parsed.fragment == '' else parsed.fragment\n",
    "    return (path, query, fragment)\n",
    "\n",
    "parentheses = re.compile(\"\\(.+\\)\")\n",
    "\n",
    "def get_relation(label):\n",
    "    \"\"\"\n",
    "    Canonicalize the relationship\n",
    "    \"\"\"\n",
    "    lbl = re.sub(parentheses, '', label)\n",
    "    l =  slugify(lbl.strip(), separator='_')\n",
    "    return l.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4.element import NavigableString as string\n",
    "from bs4.element import Tag as tag\n",
    "\n",
    "types = {\n",
    "    \"<class 'bs4.element.NavigableString'>\":\"string\",\n",
    "    \"<class 'bs4.element.Tag'>\":\"tag\"\n",
    "}\n",
    "\n",
    "infoboxes = {}\n",
    "all_properties = set()\n",
    "all_files = sorted(list(glob(html_route + \"*.html\")))\n",
    "\n",
    "for file in all_files:\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    soup:BeautifulSoup = None\n",
    "    with open(file, \"r\", encoding=\"utf8\") as r:\n",
    "        soup = BeautifulSoup(r, \"lxml\")\n",
    "        \n",
    "    wikiaMainContent = soup.find('article', {'id':'WikiaMainContent'})\n",
    "    if not wikiaMainContent:\n",
    "        continue\n",
    "    \n",
    "    infobox = wikiaMainContent.find('aside', {'class':'portable-infobox'})\n",
    "    if not infobox:\n",
    "        continue\n",
    "    \n",
    "    infoboxes[filename] = {}\n",
    "    \n",
    "    items = infobox.findAll('div', {'class': 'pi-item'})\n",
    "    for item in items:\n",
    "        h3 = item.find('h3')\n",
    "        if not h3:\n",
    "            continue\n",
    "            \n",
    "        relation = get_relation(h3.text.strip())\n",
    "        all_properties.add(relation)\n",
    "        \n",
    "        values = item.find('div', {'class':'pi-data-value'}, recursive=False)\n",
    "        infoboxes[filename][relation] = [[str(c).strip(),types.get(str(type(c)), str(type(c)))] for c in values.contents if str(c).strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"info/infoboxes.wikia.json\", \"w\", encoding=\"utf8\") as w:\n",
    "    json.dump(infoboxes, w, indent=4)\n",
    "with open(\"info/all_properties.wikia.json\", \"w\", encoding=\"utf8\") as w:\n",
    "    json.dump(list(all_properties), w, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entities %d\" % len(infoboxes))\n",
    "print(\"Possible identified relationships %d\" % len(all_properties))\n",
    "keys = list(infoboxes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_title(file):\n",
    "    soup:BeautifulSoup = None\n",
    "    with open(file, \"r\", encoding=\"utf8\") as r:\n",
    "        soup = BeautifulSoup(r, \"lxml\")\n",
    "    wikiaMainContent = soup.find('article', {'id':'WikiaMainContent'})\n",
    "    if wikiaMainContent is None:\n",
    "        return None\n",
    "    title = wikiaMainContent.get('title',None)\n",
    "    if not wikiaMainContent or not title:\n",
    "        return None\n",
    "    return wikiaMainContent['title']\n",
    "        \n",
    "all_files = sorted(list(glob(html_route + \"*.html\")))\n",
    "    \n",
    "i = 1\n",
    "entities_lst = []\n",
    "reverse = {}\n",
    "for file in all_files:\n",
    "    node = os.path.basename(file)\n",
    "    title = get_page_title(file)\n",
    "    if title is None:\n",
    "        print(file)\n",
    "        continue\n",
    "    \n",
    "    entities_lst.append([i, title, node])\n",
    "    reverse[node] = i\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "entities_df = pd.DataFrame(entities_lst, columns=['id','name','page']).set_index('id')\n",
    "entities_df.to_csv(\"info/entities.wikia.csv\", encoding=\"utf8\")\n",
    "entities_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
