{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text extraction using spaCy  \n",
    "\n",
    "The results of the execution of this notebook is a json file called `info/text_extraction.json` to be processed by [text_extraction_processing](text_extraction_processing.ipynb).  \n",
    "\n",
    "This notebook extracts depenedncies from the text, for example, consider the following string, extracted from a wikia site `Link.html`:\n",
    "\n",
    " > **Link** is the main protagonist of the [Legend of Zelda series](The_Legend_of_Zelda_series.html). He is the everlasting hero of the setting, having appeared throughout the ages in a neverending line of incarnations. The various heroes who use the name Link are courageous young boys or teenagers in [green clothing](Hero%27s_Clothes.html) who leave their homes to save the world from evil forces threatening it.\n",
    " \n",
    "The result of working with this fragment of text would be something like this:\n",
    "\n",
    "```\n",
    "{\n",
    " \"Link.html\": {\n",
    "  \"wikia\": {\n",
    "   \"name\": \"Link\",\n",
    "   \"paragraphs\": [\n",
    "    {\n",
    "     \"text\": \"Link is the main protagonist of the Legend of Zelda series. He is the everlasting hero of the setting, having appeared throughout the ages in a neverending line of incarnations. The various heroes who use the name Link are courageous young boys or teenagers in green clothing who leave their homes to save the world from evil forces threatening it.\",\n",
    "     \"links\": [\n",
    "      {\n",
    "       \"href\": \"The_Legend_of_Zelda_series.html\",\n",
    "       \"text\": \"Legend of Zelda series\"\n",
    "      }, ...\n",
    "     ],\n",
    "     \"bolds\": [\n",
    "      \"Link\"\n",
    "     ],\n",
    "     \"relations\": [\n",
    "      {\n",
    "       \"subject\": \"Link\",\n",
    "       \"relation\": \"is\",\n",
    "       \"attribute\": \"protagonist\"\n",
    "      }, ...\n",
    "     ],\n",
    "     \"details\": [\n",
    "      {\n",
    "       \"attribute\": \"protagonist\",\n",
    "       \"relation\": \"of\",\n",
    "       \"subject\": \"the Legend of Zelda\"\n",
    "      }, ...\n",
    "     ]\n",
    "    }\n",
    "   ]\n",
    "  }\n",
    " }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from slugify import slugify\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import unquote\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from extracted_entities import ParsedParagraph, Relation, RelationDetails, ExtractedEncoder\n",
    "from ie_conf import get_htmls_route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load information from disk and merge it into a single dataframe. Also, define some important info about the paths of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {\n",
    "    'gamepedia': get_htmls_route(\"gamepedia\"),\n",
    "    'wikia': get_htmls_route(\"wikia\")\n",
    "}\n",
    "\n",
    "wikia = pd.read_csv(\"info/entities.wikia.csv\", \n",
    "                    names=[\"id\", \"name\", \"url\"],\n",
    "                    usecols=[\"name\", \"url\"], \n",
    "                    header=0, index_col=[\"url\"])\n",
    "gamepedia = pd.read_csv(\"info/entities.gamepedia.csv\", \n",
    "                        names=[\"id\", \"name\", \"url\"],\n",
    "                        usecols=[\"name\", \"url\"], \n",
    "                        header=0, index_col=[\"url\"])\n",
    "\n",
    "grouped = pd.merge(wikia, gamepedia, \n",
    "                   left_index=True, right_index=True,\n",
    "                   suffixes=[\"_wikia\",\"_gamepedia\"], \n",
    "                   how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a Doc object. The most common way to get a Doc object is via the nlp object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting *specific* dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Link is the main protagonist of the Legend of Zelda series. He is the everlasting hero of \" + \\\n",
    "\"the setting, having appeared throughout the ages in a neverending line of incarnations. \" + \\\n",
    "\"Veil Springs is a location from The Legend of Zelda\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "def debug_token(word, indent=0):\n",
    "    print((\"\\t\" * indent) + str(word), \n",
    "          word.dep_, \n",
    "          \"None\" if word.ent_type_ == None else word.ent_type_)\n",
    "    \n",
    "def get_dependencies(sent):\n",
    "    relations =[]\n",
    "    details = []\n",
    "    doc = nlp(sent)\n",
    "    for ent in doc.ents:\n",
    "        ent.merge(tag=ent.root.tag_, lemma=ent.text, ent_type=ent.label_)\n",
    "\n",
    "    for word in doc: # word is spacy.tokens.token.Token\n",
    "        if word.dep_ in ('attr'): # dep_ is Syntactic dependency relation\n",
    "            attr = word\n",
    "            relation = word.head # The syntactic parent, or \"governor\", of 'attr'.\n",
    "            for subject in relation.lefts: # The leftward immediate children of the 'parent'\n",
    "                relations.append(Relation(subject, relation, attr, subject.idx))\n",
    "        elif word.dep_ == 'pobj':\n",
    "            subject = word\n",
    "            relation = word.head\n",
    "            attr = relation.head\n",
    "            if attr.dep_ == 'attr':\n",
    "                details.append(RelationDetails(attr, relation, subject, subject.idx))\n",
    "\n",
    "    return relations, details\n",
    "\n",
    "for s in sentences:\n",
    "    rels, dets = get_dependencies(s)\n",
    "    for rel in rels:\n",
    "        print(rel)\n",
    "    for det in dets:\n",
    "        print(det)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operate with our specific wikis case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spac_s = re.compile(\"\\s+([\\,\\.\\?\\!]{1})\")\n",
    "spaces = re.compile(\"\\s+\")\n",
    "japs = re.compile(\"\\(.*[ぁ-んァ-ン]+.+\\)\\s\")\n",
    "sqbr = re.compile(\"\\[[0-9a-z\\s]+\\]\")\n",
    "\n",
    "def clean_string(label):\n",
    "    \"\"\"Clean string removing all special characters\"\"\"\n",
    "    st = label\n",
    "    st = st.replace('\"', '')\n",
    "    st = re.sub(spac_s, '\\g<1>', st)\n",
    "    st = re.sub(spaces, ' ', st)\n",
    "    st = re.sub(japs, '', st)\n",
    "    st = re.sub(sqbr, '', st)\n",
    "    return st.strip()\n",
    "\n",
    "def extract_p_features(p):\n",
    "    links_ = p.findAll('a')\n",
    "    links = []\n",
    "    if links_:\n",
    "        links = [{'href':anchor.get('href', '#'), 'text':clean_string(anchor.text)} \n",
    "                 for anchor in links_ if not anchor.get('href', '#').startswith(\"../../\")]\n",
    "    bolds_ = p.find('b')\n",
    "    bolds = []\n",
    "    if bolds_:\n",
    "        bolds = [clean_string(str(b)) for b in bolds_]\n",
    "    txt = clean_string(p.text)\n",
    "    return ParsedParagraph(txt, links, bolds)\n",
    "\n",
    "def extract_paragraphs(file, num_pharagraphs=1):\n",
    "    page:BeautifulSoup = None\n",
    "    with open(file, \"r\", encoding=\"utf8\") as r:\n",
    "        page = BeautifulSoup(r, \"lxml\")\n",
    "    content = page.find('div', {'id':'mw-content-text'})\n",
    "    ps = content.findAll('p', recursive=False)\n",
    "    paragraphs = []\n",
    "    for i in range(min(len(ps),num_pharagraphs)):\n",
    "        paragraphs.append(extract_p_features(ps[i]))\n",
    "    return paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = grouped.sample(2)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_rels_from_df(dataframe, paragraph_count=1):\n",
    "    extracted = {}\n",
    "\n",
    "    for r in dataframe.iterrows():\n",
    "        resource = r[0]\n",
    "        extracted[resource] = { }\n",
    "        for source in sources:\n",
    "            if pd.notna(r[1][\"name_\" + source]):\n",
    "                f = os.path.join(sources[source], resource)\n",
    "                if not os.path.exists(f):\n",
    "                    continue\n",
    "                paragraphs = []\n",
    "                extracted_paragraphs = extract_paragraphs(f,paragraph_count)\n",
    "                for paragraph in extracted_paragraphs:\n",
    "                    sentences = sent_tokenize(paragraph.text)\n",
    "                    relations = []\n",
    "                    details = []\n",
    "                    for s in sentences:\n",
    "                        rels, dets = get_dependencies(s)\n",
    "                        if rels:\n",
    "                            relations.extend(rels)\n",
    "                        if dets:\n",
    "                            details.extend(dets)\n",
    "                            \n",
    "                    if relations:\n",
    "                        paragraph.relations = relations\n",
    "                    if details:\n",
    "                        paragraph.details = details\n",
    "                    paragraphs.append(paragraph)\n",
    "                extracted[resource][source] = {\n",
    "                    'name': r[1][\"name_\" + source]\n",
    "                }\n",
    "                extracted[resource][source][\"paragraphs\"] = paragraphs\n",
    "    return extracted\n",
    "\n",
    "extracted = get_rels_from_df(sample)\n",
    "print(json.dumps(extracted, indent=1,cls=ExtractedEncoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now... process all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = get_rels_from_df(grouped)\n",
    "with open(\"info/text_extraction.json\", \"w\") as w:\n",
    "    json.dump(extracted, w, indent=1,cls=ExtractedEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
